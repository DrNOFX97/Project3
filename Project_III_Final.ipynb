{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install os re json time wave yt-dlp sqlite3 pinecone-client requests tiktoken subprocess tqdm datasets vosk langchain langchain-pinecone langchain-openai tabulate ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNjSkEtrtU_O",
    "outputId": "47f09885-fe0e-4484-9ca9-da8fca151e80"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import wave\n",
    "import yt_dlp\n",
    "import sqlite3\n",
    "import pinecone\n",
    "import requests\n",
    "import tiktoken\n",
    "import subprocess\n",
    "\n",
    "from uuid import uuid4\n",
    "from typing import List\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "from vosk import Model, KaldiRecognizer\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.agents import Tool, create_react_agent\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI, OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "\n",
    "from langchain.output_parsers import ListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from vosk import Model, KaldiRecognizer\n",
    "from tabulate import tabulate\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNjSkEtrtU_O",
    "outputId": "47f09885-fe0e-4484-9ca9-da8fca151e80"
   },
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set API Keys (ensure these are set in your .env file)\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "\n",
    "# Ensure ffmpeg is in PATH\n",
    "os.environ['PATH'] += os.pathsep + '/usr/local/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNjSkEtrtU_O",
    "outputId": "47f09885-fe0e-4484-9ca9-da8fca151e80"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the YouTube video URL:  What is a qubit?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid YouTube URL. Please enter a valid URL.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the YouTube video URL:  https://www.youtube.com/watch?v=qQviI1d_hFA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting YouTube video info...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.ytimg.com/vi/qQviI1d_hFA/maxresdefault.jpg\" width=\"580\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YouTube Video Info:\n",
      "+-------------+-------------------------------------------------------+\n",
      "| Field       | Value                                                 |\n",
      "+=============+=======================================================+\n",
      "| Title       | Michio Kaku: Quantum computing is the next revolution |\n",
      "+-------------+-------------------------------------------------------+\n",
      "| Uploader    | Big Think                                             |\n",
      "+-------------+-------------------------------------------------------+\n",
      "| Uploader ID | @bigthink                                             |\n",
      "+-------------+-------------------------------------------------------+\n",
      "| Upload Date | 20230818                                              |\n",
      "+-------------+-------------------------------------------------------+\n",
      "| Duration    | 677                                                   |\n",
      "+-------------+-------------------------------------------------------+\n",
      "| View Count  | 2140713                                               |\n",
      "+-------------+-------------------------------------------------------+\n",
      "| Like Count  | 52577                                                 |\n",
      "+-------------+-------------------------------------------------------+\n",
      "Downloading video...\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=qQviI1d_hFA\n",
      "[youtube] qQviI1d_hFA: Downloading webpage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube] Webpage contains broken formats (poToken experiment detected). Ignoring initial player response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] qQviI1d_hFA: Downloading ios player API JSON\n",
      "[youtube] qQviI1d_hFA: Downloading player 5604538d\n",
      "[youtube] qQviI1d_hFA: Downloading web player API JSON\n",
      "[youtube] qQviI1d_hFA: Downloading m3u8 information\n",
      "[info] qQviI1d_hFA: Downloading 1 format(s): 251\n",
      "[download] Destination: audio.webm\n",
      "[download] 100% of   10.70MiB in 00:00:00 at 20.26MiB/s    \n",
      "[ExtractAudio] Destination: audio.wav\n",
      "Deleting original file audio.webm (pass -k to keep)\n",
      "Transcribing audio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from model/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from model/graph/HCLr.fst model/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo model/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting audio to the correct format...\n",
      "Audio converted successfully: converted_audio.wav\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbb4f81251843edb6d98249567ce143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transcribing:   0%|          | 0/10836846 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcription:\n",
      "Transcription saved to 'transcription.txt'.\n"
     ]
    }
   ],
   "source": [
    "def is_valid_youtube_url(url):\n",
    "    \"\"\"Validate if the given URL is a YouTube URL.\"\"\"\n",
    "    pattern = r'^(https?://)?(www\\.)?(youtube\\.com|youtu\\.?be)/.+$'\n",
    "    return re.match(pattern, url) is not None\n",
    "\n",
    "def download_video(url):\n",
    "    \"\"\"Download the audio of the YouTube video as a .wav file.\"\"\"\n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'wav',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'outtmpl': 'audio.%(ext)s',\n",
    "        'ffmpeg_location': '/usr/local/bin'\n",
    "    }\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([url])\n",
    "        return 'audio.wav'\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during download: {e}\")\n",
    "        return None\n",
    "\n",
    "def convert_audio(input_file, output_file):\n",
    "    \"\"\"Convert audio to the required format for transcription.\"\"\"\n",
    "    command = [\n",
    "        '/usr/local/bin/ffmpeg',\n",
    "        '-i', input_file,\n",
    "        '-acodec', 'pcm_s16le',\n",
    "        '-ac', '1',\n",
    "        '-ar', '16000',\n",
    "        '-y', output_file\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(f\"Audio converted successfully: {output_file}\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error converting audio: {e}\")\n",
    "        return False\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    \"\"\"Transcribe audio using the Vosk model.\"\"\"\n",
    "    model_path = \"model\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"Speech recognition model not found. Please make sure you've downloaded it.\")\n",
    "        return None\n",
    "\n",
    "    model = Model(model_path)\n",
    "    recognizer = KaldiRecognizer(model, 16000)\n",
    "\n",
    "    def is_valid_wave_file(wave_file):\n",
    "        \"\"\"Check if the wave file has the correct properties.\"\"\"\n",
    "        return (wave_file.getnchannels() == 1 and\n",
    "                wave_file.getsampwidth() == 2 and\n",
    "                wave_file.getcomptype() == \"NONE\")\n",
    "    \n",
    "    try:\n",
    "        # Open the original file to check format\n",
    "        with wave.open(audio_file, \"rb\") as wf:\n",
    "            if not is_valid_wave_file(wf):\n",
    "                print(\"Converting audio to the correct format...\")\n",
    "                converted_file = \"converted_audio.wav\"\n",
    "                if not convert_audio(audio_file, converted_file):\n",
    "                    return None\n",
    "                audio_file = converted_file\n",
    "\n",
    "        # Reopen the audio file (either original or converted)\n",
    "        with wave.open(audio_file, \"rb\") as wf:\n",
    "            results = []\n",
    "            total_frames = wf.getnframes()\n",
    "            with tqdm(total=total_frames, desc=\"Transcribing\") as pbar:\n",
    "                while True:\n",
    "                    data = wf.readframes(4000)\n",
    "                    if not data:\n",
    "                        break\n",
    "                    if recognizer.AcceptWaveform(data):\n",
    "                        part_result = json.loads(recognizer.Result())\n",
    "                        results.append(part_result.get('text', ''))\n",
    "                    pbar.update(4000)\n",
    "\n",
    "                # Final result\n",
    "                part_result = json.loads(recognizer.FinalResult())\n",
    "                results.append(part_result.get('text', ''))\n",
    "\n",
    "            transcription = \" \".join(results)\n",
    "            if not transcription.strip():\n",
    "                print(\"No transcription result obtained. Please check the audio file and model.\")\n",
    "            return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during transcription: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_youtube_video_info(url):\n",
    "    \"\"\"Extract YouTube video info and M3U8 URLs without downloading the video.\"\"\"\n",
    "    ydl_opts = {\n",
    "        'quiet': True,\n",
    "        'skip_download': True,\n",
    "        'extract_flat': False,\n",
    "        'force_generic_extractor': True,\n",
    "        'no_warnings': True\n",
    "    }\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(url, download=False)\n",
    "            info.pop('description', None)\n",
    "\n",
    "            # Extract M3U8 URLs from formats\n",
    "            formats = info.get('formats', [])\n",
    "            m3u8_urls = [fmt['url'] for fmt in formats if 'm3u8' in fmt.get('url', '')]\n",
    "            info['m3u8_urls'] = m3u8_urls\n",
    "\n",
    "        return info\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while extracting YouTube video info: {e}\")\n",
    "        return None\n",
    "\n",
    "def display_youtube_info(info):\n",
    "    \"\"\"Display YouTube video metadata in a pretty table format using tabulate.\"\"\"\n",
    "    if not info:\n",
    "        print(\"No YouTube video info to display.\")\n",
    "        return\n",
    "\n",
    "    youtube_info = [\n",
    "        [\"Title\", info.get(\"title\", \"N/A\")],\n",
    "        [\"Uploader\", info.get(\"uploader\", \"N/A\")],\n",
    "        [\"Uploader ID\", info.get(\"uploader_id\", \"N/A\")],\n",
    "        [\"Upload Date\", info.get(\"upload_date\", \"N/A\")],\n",
    "        [\"Duration\", info.get(\"duration\", \"N/A\")],\n",
    "        [\"View Count\", info.get(\"view_count\", \"N/A\")],\n",
    "        [\"Like Count\", info.get(\"like_count\", \"N/A\")]\n",
    "    ]\n",
    "\n",
    "    thumbnail_url = info.get(\"thumbnail\")\n",
    "    if thumbnail_url:\n",
    "        display(Image(url=thumbnail_url, width=580))\n",
    "    \n",
    "    print(\"\\nYouTube Video Info:\")\n",
    "    print(tabulate(youtube_info, headers=[\"Field\", \"Value\"], tablefmt=\"grid\", stralign=\"left\"))\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to handle the workflow.\"\"\"\n",
    "    while True:\n",
    "        video_url = input(\"Enter the YouTube video URL: \")\n",
    "        if is_valid_youtube_url(video_url):\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid YouTube URL. Please enter a valid URL.\")\n",
    "\n",
    "    print(\"Extracting YouTube video info...\")\n",
    "    youtube_info = get_youtube_video_info(video_url)\n",
    "    if youtube_info:\n",
    "        display_youtube_info(youtube_info)\n",
    "\n",
    "    print(\"Downloading video...\")\n",
    "    audio_file = download_video(video_url)\n",
    "\n",
    "    if audio_file and os.path.exists(audio_file):\n",
    "        print(\"Transcribing audio...\")\n",
    "        transcription = transcribe_audio(audio_file)\n",
    "\n",
    "        if transcription:\n",
    "            print(\"\\nTranscription:\")\n",
    "            #print(transcription)\n",
    "\n",
    "            # Save transcription to a file\n",
    "            with open('transcription.txt', 'w') as f:\n",
    "                f.write(transcription)\n",
    "\n",
    "            print(\"Transcription saved to 'transcription.txt'.\")\n",
    "        else:\n",
    "            print(\"Transcription failed. Please check the error messages above.\")\n",
    "    else:\n",
    "        print(\"Failed to download the video. Please check the URL and try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully loaded into the database.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the transcription file\n",
    "transcription_file = 'transcription.txt'\n",
    "\n",
    "# Read the transcription text\n",
    "with open(transcription_file, 'r') as file:\n",
    "    transcription_text = file.read()\n",
    "\n",
    "# Connect to SQLite database (or create it if it doesn't exist)\n",
    "conn = sqlite3.connect('transcriptions.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the transcriptions table if it does not exist\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS transcriptions (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        speaker TEXT,\n",
    "        text TEXT,\n",
    "        timestamp TEXT\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Insert the transcription text into the table\n",
    "cursor.execute('''\n",
    "    INSERT INTO transcriptions (speaker, text, timestamp)\n",
    "    VALUES (?, ?, ?)\n",
    "''', ('Transcript', transcription_text, '2024-07-15 10:00:00'))\n",
    "\n",
    "# Commit the transaction and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# Data has been successfully loaded into the database\n",
    "print(\"Data has been successfully loaded into the database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'o200k_base'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken.encoding_for_model('gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "we all know the digital computers changed virtually every aspect of our life well the arrival of quantum computers could be even more historic than that we're now and the initial stages of the next revolution we're talking about a new generation of computers the ultimate computer a computer that computer on adam the ultimate constituents of batter itself the question is who's involved in this race to perfect quantum computers and the answer is everyone all the big players are part of this race because if they're not silicon valley could become the next rust belt also anyone who's interested in security is interested in the quantum computers they can crack almost any code that is based on digital technology that's why the f b i the cia and all national governments are following this is very closely what their computers will change everything the economy how he saw problems the way we interact with the universe you name it won't to computers will be there i'm doctor or michio kaku professor of theoretical physics at the city university of new york and author of quantum supremacy about the rise of quantum computers if he computers have gone through three basic stages stage one was the analog computer so two thousand years ago there was a shipwreck and in the boat that sank was a device and when you brushed away a the dirt and debris you began to realize that it was a machine a machine of incredible complexity he was in fact the world's first analog computer and it was designed to map the motion of the moon the sun and the planet's to simulate the universe but as we primitive people's became more prosperous we had to count things count how many cows you had count how much profit you may analog computers could be based on sticks bones whatever it took the count so this went on for thousands of years until finally we reach the work of charles babbage he creates the ultimate analog computer with hundreds of years and lovers and bullies and by turning the crank you can then calculate longitude latitude you can calculate interest rates it was very valuable to have it is made like that where the banking industry for commerce then world war two comes along babbage's machine is simply too primitive rated the german code so the job was given to mathematicians like alan turing alan turing was the one who codified lot of the laws of computation into what it's called a turing machine and of course is digital know the digital revolution is based on transistors it operates on zeros and ones zeros and ones at the speed of electricity every digital computer is a turing machine the next step beyond digital computers is the quantum era where to find it was one of the founders of quantum electrodynamics but also a visionary and he asked himself a simple question how small can you make\n",
      "\n",
      "Chunk 2:\n",
      "founders of quantum electrodynamics but also a visionary and he asked himself a simple question how small can you make a transistor and he realize that the old transistor is an atom one atom that could control the flow of electricity not just on or off but everything in between we have to go to quantum computers computers their computer on adams rather than on transistors transistors are based on zeros zero one zero zero one reality is not reality is based on electrons and particles and these particles in turn act like waves so you have to have a new set of mathematics to discuss the way leaves that make up a molecule and that's where quantum computers come in they're based on electrons and these electrons how can they have so much computational power because they could be in two places at the same time that's what gives quantum computers their power they compute on not just one universe but an infinite number of parallel universes at the fundamental level quantum mechanics can be reduced down to a cat choate injuries cat let's take a box in the box you put a cat and the question is is the cat dead or alive well until you open the box you don't know it is alive and dead simultaneously is in a superposition of to states in other words the universe has split in half in one have the cat is alive and the other universe they get is dead that's the basis of the quantum theory that until you make a measurement the cat can exists in both states simultaneously in fact in any number of states simultaneously the cat can be dead or alive play ing jumping sideways sick any number of states now why am i mentioning this is this summarizes the power of quantum computers weren't to computers compute on parallel universes that's why they are so powerful so how much faster is a quantum computer over a digital computer in principle infinitely faster when we talk about digital computers we can measure their power in terms of bits for example spin up spin down zeros and one would constitute one bit for a large is your computer we're not talking about billions of bits that are modeled by train sisters except now quantum computers talk not just about spin up or a spin down but everything in between that's called a cuban one cupid represents all the possibilities have an object spinning between up and down thousands of cuba's can now be modeled with the latest generation of quantum computers eventually we hope to hit a million and so we're talking about exceeding the power of ordinary digital computers that's called quantum supremacy it is the point at which a quantum computer can out race and are perform a digital computer on a certain task we pass that several years ago but we want a machine that could exceed the power of any digital\n",
      "\n",
      "Chunk 3:\n",
      "task we pass that several years ago but we want a machine that could exceed the power of any digital computer we're not there yet or very close to it the number one problem facing quantum computers is the question of d coherence everything is based on particles like electrons and electrons have waves associate with them when these waves are vibrating in unison as call coherence and then you can do calculations of a quantum mechanical nature but if you fall out of coherence that everyday vibrates at a different frequency and what is that called noise you have to reduce the temperature down to near absolute zero so everything is pretty much vibrating slowly in unison that's difficult now nature solves this problem it is the basis of all life on the earth photosynthesis for example is a quantum mechanical process mother nature can create coherence at room temperature amazing a flower can do calculations that are most advanced quantum computer cannot mother nature is still smarter than us when it comes to the quantum theory so let's face it there are hurdles affecting the growth of quantum computers but they pale in comparison to the benefits that may be unleashed by quantum computers were talking about opening the floodgates take a look for example of food supply the green revolution then allows us to feed the population of the world is slowly coming to an end we're trying to use quantum computers to unlock the secrets of how to make fertilizer from nitrogen take a look at energy quantum computers may be able to create fusion power by stabilizing the super hot hydrogen inside a fusion reactor and take a look at medicine you realize that life is based on molecules molecules they can create alzheimer's disease parkinson's disease cancer diseases are beyond the reach of digital computers but hey this is what decline to computers do will be able to model diseases at the molecular level and that's why we hope to cure the incurable using quantum computers were talking about turning medicine up i down my personal hope for quantum computers is that will be able to create a theory of the entire universe the theory that eluded einstein the theory that would explain black holes and supernovas and galactic evolution but the equations are so complex that know one no one has been able to solve them perhaps they'll be solved in the memory of a quantum computer   to learn even more from the world's biggest thinkers get big think plus for your business\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the length of text in terms of tokens\n",
    "def tiktoken_len(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('transcriptions.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query the data\n",
    "cursor.execute('SELECT text FROM transcriptions WHERE id = 1')\n",
    "transcription_text = cursor.fetchone()[0]\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20,\n",
    "    length_function=tiktoken_len\n",
    ")\n",
    "\n",
    "# Split the transcription text into chunks\n",
    "chunks = text_splitter.split_text(transcription_text)[:3]\n",
    "\n",
    "# Print the first 3 chunks\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500, 430)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_len(chunks[0]), tiktoken_len(chunks[1]), tiktoken_len(chunks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'text-embedding-3-small'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=openai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 17838}},\n",
       " 'total_vector_count': 17838}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configure client\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-east-1\"\n",
    ")\n",
    "index_name = 'langchain-retrieval-augmentation'\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # dimensionality of ada 002\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a938c2f1ba34a968e67cbc41568af9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing completed.\n"
     ]
    }
   ],
   "source": [
    "batch_limit=50\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len  # You can replace with your custom token length function if needed\n",
    ")\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect('transcriptions.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch data from SQLite database\n",
    "cursor.execute('SELECT id, speaker, text FROM transcriptions')\n",
    "data = cursor.fetchall()\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Initialize lists for texts and metadatas\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "# Process each record fetched from the database\n",
    "for i, record in enumerate(tqdm(data)):\n",
    "    # Metadata fields for this record\n",
    "    metadata = {\n",
    "        #'index': str(record[0]),  # Assuming id is the first column\n",
    "        'speaker': record[1],  # Replace with actual source if available\n",
    "        'text': record[2],  # Example title based on id\n",
    "    }\n",
    "\n",
    "    # Split text into chunks\n",
    "    record_texts = text_splitter.split_text(record[2])  # Assuming text is the third column\n",
    "\n",
    "    # Create individual metadata dicts for each chunk\n",
    "    record_metadatas = [{\n",
    "        \"chunk\": j,\n",
    "        \"text\": text,\n",
    "        **metadata\n",
    "    } for j, text in enumerate(record_texts)]\n",
    "\n",
    "    # Append texts and metadatas to current batches\n",
    "    texts.extend(record_texts)\n",
    "    metadatas.extend(record_metadatas)\n",
    "\n",
    "    # Check if batch limit is reached, then embed and upsert\n",
    "    if len(texts) >= batch_limit:\n",
    "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        embeds = embed.embed_documents(texts)\n",
    "\n",
    "        # Assuming `index` is where you want to upsert (not defined in the snippet)\n",
    "        index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "\n",
    "        # Clear lists after upserting\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "\n",
    "# Process any remaining texts in the lists\n",
    "if len(texts) > 0:\n",
    "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "    embeds = embed.embed_documents(texts)\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "\n",
    "print(\"Data processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 17838}},\n",
       " 'total_vector_count': 17838}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Pinecone Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your query:  What is a qubit?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is a qubit?',\n",
       " 'result': 'A qubit, or quantum bit, is the basic unit of quantum information. Unlike a classical bit, which can be either 0 or 1, a qubit can exist in a superposition of both states simultaneously. This means that a qubit can represent multiple possibilities at once, allowing quantum computers to perform complex calculations more efficiently than classical computers. The state of a qubit can be described using quantum mechanics, and it can also be entangled with other qubits, leading to even more powerful computational capabilities.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set API Keys\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Define the Pinecone index and embedding model\n",
    "embed_model = OpenAIEmbeddings(model='text-embedding-3-small', openai_api_key=openai_api_key)\n",
    "\n",
    "# Connect to the Pinecone index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Initialize the Pinecone vector store object (assuming no text_field parameter is required)\n",
    "vectorstore = PineconeVectorStore(index, embed_model)\n",
    "\n",
    "# Get user input for the query\n",
    "query = input(\"Please enter your query: \")\n",
    "\n",
    "# Perform similarity search\n",
    "try:\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the similarity search: {e}\")\n",
    "    results = []\n",
    "\n",
    "# Connect to SQLite database (or create it if it doesn't exist)\n",
    "conn = sqlite3.connect('search_results.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Drop the table if it exists\n",
    "cursor.execute('DROP TABLE IF EXISTS search_results')\n",
    "\n",
    "# Create the search_results table with the correct schema\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS search_results (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        query TEXT,\n",
    "        document_id TEXT,\n",
    "        score REAL\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Insert the search results into the table\n",
    "for result in results:\n",
    "    try:\n",
    "        # Adjust according to the structure of `result`\n",
    "        document_id = getattr(result, 'id', 'unknown')  # Default to 'unknown' if 'id' is missing\n",
    "        score = getattr(result, 'score', 0.0)          # Default to 0.0 if 'score' is missing\n",
    "\n",
    "        cursor.execute('''\n",
    "            INSERT INTO search_results (query, document_id, score)\n",
    "            VALUES (?, ?, ?)\n",
    "        ''', (query, document_id, score))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while inserting results into the database: {e}\")\n",
    "\n",
    "# Commit the transaction and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# Retrieve and set memory\n",
    "\n",
    "# completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    "    )\n",
    "\n",
    "# retrieval qa chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3e4a1\" style=\"font-size: 14px; text-align: center;\">\n",
       "  <caption>Evaluation Metrics</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_3e4a1_level0_col0\" class=\"col_heading level0 col0\" >Metric</th>\n",
       "      <th id=\"T_3e4a1_level0_col1\" class=\"col_heading level0 col1\" >Score (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_3e4a1_row0_col0\" class=\"data row0 col0\" >Accuracy</td>\n",
       "      <td id=\"T_3e4a1_row0_col1\" class=\"data row0 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3e4a1_row1_col0\" class=\"data row1 col0\" >Precision</td>\n",
       "      <td id=\"T_3e4a1_row1_col1\" class=\"data row1 col1\" >100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3e4a1_row2_col0\" class=\"data row2 col0\" >Recall</td>\n",
       "      <td id=\"T_3e4a1_row2_col1\" class=\"data row2 col1\" >38.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3e4a1_row3_col0\" class=\"data row3 col0\" >Accuracy (Token-Based)</td>\n",
       "      <td id=\"T_3e4a1_row3_col1\" class=\"data row3 col1\" >38.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3e4a1_row4_col0\" class=\"data row4 col0\" >F1 Score</td>\n",
       "      <td id=\"T_3e4a1_row4_col1\" class=\"data row4 col1\" >48.275862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3e4a1_row5_col0\" class=\"data row5 col0\" >Token-Based Accuracy</td>\n",
       "      <td id=\"T_3e4a1_row5_col1\" class=\"data row5 col1\" >38.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3e4a1_row6_col0\" class=\"data row6 col0\" >BLEU Score</td>\n",
       "      <td id=\"T_3e4a1_row6_col1\" class=\"data row6 col1\" >12.201070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3e4a1_row7_col0\" class=\"data row7 col0\" >Context Precision</td>\n",
       "      <td id=\"T_3e4a1_row7_col1\" class=\"data row7 col1\" >66.170635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3e4a1_row8_col0\" class=\"data row8 col0\" >Context Recall</td>\n",
       "      <td id=\"T_3e4a1_row8_col1\" class=\"data row8 col1\" >39.580247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3e4a1_row9_col0\" class=\"data row9 col0\" >Hallucination Score</td>\n",
       "      <td id=\"T_3e4a1_row9_col1\" class=\"data row9 col1\" >33.829365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_3e4a1_row10_col0\" class=\"data row10 col0\" >Answer Correctness</td>\n",
       "      <td id=\"T_3e4a1_row10_col1\" class=\"data row10 col1\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x3086f2e40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error Analysis:\n",
      "Query: What is a qubit?\n",
      "True Answer: A qubit is the basic unit of quantum information, analogous to a bit in classical computing, but capable of being in a superposition of states.\n",
      "Model Answer: A qubit is a unit of quantum information, similar to a bit but with more capabilities.\n",
      "\n",
      "Query: Explain quantum entanglement.\n",
      "True Answer: Quantum entanglement is a physical phenomenon where particles become correlated in such a way that the state of one particle instantly influences the state of the other, no matter the distance between them.\n",
      "Model Answer: Quantum entanglement is a phenomenon where particles are interconnected and the state of one affects the state of another.\n",
      "\n",
      "Query: How does a quantum computer work?\n",
      "True Answer: A quantum computer uses qubits and quantum gates to perform computations that can solve certain problems more efficiently than classical computers by leveraging principles of quantum mechanics.\n",
      "Model Answer: Quantum computers use qubits to perform complex calculations that classical computers struggle with.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "# Define sample queries and reference answers\n",
    "queries = [\n",
    "    \"What is a qubit?\",\n",
    "    \"Explain quantum entanglement.\",\n",
    "    \"How does a quantum computer work?\"\n",
    "]\n",
    "\n",
    "reference_answers = {\n",
    "    \"What is a qubit?\": \"A qubit is the basic unit of quantum information, analogous to a bit in classical computing, but capable of being in a superposition of states.\",\n",
    "    \"Explain quantum entanglement.\": \"Quantum entanglement is a physical phenomenon where particles become correlated in such a way that the state of one particle instantly influences the state of the other, no matter the distance between them.\",\n",
    "    \"How does a quantum computer work?\": \"A quantum computer uses qubits and quantum gates to perform computations that can solve certain problems more efficiently than classical computers by leveraging principles of quantum mechanics.\"\n",
    "}\n",
    "\n",
    "model_responses = {\n",
    "    \"What is a qubit?\": \"A qubit is a unit of quantum information, similar to a bit but with more capabilities.\",\n",
    "    \"Explain quantum entanglement.\": \"Quantum entanglement is a phenomenon where particles are interconnected and the state of one affects the state of another.\",\n",
    "    \"How does a quantum computer work?\": \"Quantum computers use qubits to perform complex calculations that classical computers struggle with.\"\n",
    "}\n",
    "\n",
    "# Scoring functions\n",
    "def calculate_accuracy(reference_answers, model_responses):\n",
    "    \"\"\"\n",
    "    Calculate accuracy based on exact matches between model responses and reference answers.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = len(reference_answers)\n",
    "    for query, ref_answer in reference_answers.items():\n",
    "        model_answer = model_responses.get(query, \"\")\n",
    "        if model_answer.strip() == ref_answer.strip():\n",
    "            correct += 1\n",
    "    return (correct / total) * 100\n",
    "\n",
    "def calculate_precision_recall_accuracy(reference_answers, model_responses):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and accuracy based on token-level comparison.\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for query, ref_answer in reference_answers.items():\n",
    "        model_answer = model_responses.get(query, \"\")\n",
    "        ref_tokens = set(ref_answer.lower().split())\n",
    "        model_tokens = set(model_answer.lower().split())\n",
    "\n",
    "        y_true.extend([1] * len(ref_tokens))\n",
    "        y_pred.extend([1 if token in model_tokens else 0 for token in ref_tokens])\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return (precision * 100, recall * 100, accuracy * 100)\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, F1 score, and token-based accuracy based on predictions and references.\n",
    "    \"\"\"\n",
    "    all_pred_tokens = [token for pred in predictions for token in pred.split()]\n",
    "    all_ref_tokens = [token for ref in references for token in ref.split()]\n",
    "\n",
    "    pred_tokens = set(all_pred_tokens)\n",
    "    ref_tokens = set(all_ref_tokens)\n",
    "\n",
    "    precision = len(pred_tokens.intersection(ref_tokens)) / len(pred_tokens) if pred_tokens else 0\n",
    "    recall = len(pred_tokens.intersection(ref_tokens)) / len(ref_tokens) if ref_tokens else 0\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    token_based_accuracy = len(pred_tokens.intersection(ref_tokens)) / len(ref_tokens) if ref_tokens else 0\n",
    "\n",
    "    return {\n",
    "        'precision': precision * 100,\n",
    "        'recall': recall * 100,\n",
    "        'f1_score': f1 * 100,\n",
    "        'token_based_accuracy': token_based_accuracy * 100\n",
    "    }\n",
    "\n",
    "def compute_context_precision_recall(reference_answers, model_responses):\n",
    "    \"\"\"\n",
    "    Compute context precision and recall based on the presence of relevant context words.\n",
    "    \"\"\"\n",
    "    context_precision_scores = []\n",
    "    context_recall_scores = []\n",
    "\n",
    "    for query, ref_answer in reference_answers.items():\n",
    "        model_answer = model_responses.get(query, \"\")\n",
    "        ref_tokens = set(ref_answer.lower().split())\n",
    "        model_tokens = set(model_answer.lower().split())\n",
    "\n",
    "        if model_tokens:\n",
    "            context_precision = len(ref_tokens.intersection(model_tokens)) / len(model_tokens)\n",
    "        else:\n",
    "            context_precision = 0\n",
    "        \n",
    "        if ref_tokens:\n",
    "            context_recall = len(ref_tokens.intersection(model_tokens)) / len(ref_tokens)\n",
    "        else:\n",
    "            context_recall = 0\n",
    "        \n",
    "        context_precision_scores.append(context_precision)\n",
    "        context_recall_scores.append(context_recall)\n",
    "\n",
    "    avg_context_precision = np.mean(context_precision_scores) * 100\n",
    "    avg_context_recall = np.mean(context_recall_scores) * 100\n",
    "\n",
    "    return avg_context_precision, avg_context_recall\n",
    "\n",
    "def compute_hallucination(reference_answers, model_responses):\n",
    "    \"\"\"\n",
    "    Compute hallucination score based on the presence of irrelevant information.\n",
    "    \"\"\"\n",
    "    hallucination_scores = []\n",
    "\n",
    "    for query, ref_answer in reference_answers.items():\n",
    "        model_answer = model_responses.get(query, \"\")\n",
    "        ref_tokens = set(ref_answer.lower().split())\n",
    "        model_tokens = set(model_answer.lower().split())\n",
    "\n",
    "        hallucinated_tokens = model_tokens - ref_tokens\n",
    "        hallucination_score = len(hallucinated_tokens) / len(model_tokens) if model_tokens else 0\n",
    "        hallucination_scores.append(hallucination_score)\n",
    "\n",
    "    avg_hallucination_score = np.mean(hallucination_scores) * 100\n",
    "    return avg_hallucination_score\n",
    "\n",
    "def compute_answer_correctness(reference_answers, model_responses):\n",
    "    \"\"\"\n",
    "    Compute answer correctness based on exact matches between model responses and reference answers.\n",
    "    \"\"\"\n",
    "    correct_count = 0\n",
    "    total_count = len(reference_answers)\n",
    "\n",
    "    for query, ref_answer in reference_answers.items():\n",
    "        model_answer = model_responses.get(query, \"\")\n",
    "        if model_answer.strip() == ref_answer.strip():\n",
    "            correct_count += 1\n",
    "\n",
    "    answer_correctness = (correct_count / total_count) * 100\n",
    "    return answer_correctness\n",
    "\n",
    "# Sample data for compute_metrics function\n",
    "predictions = [\n",
    "    \"The qubit is the basic unit of quantum information.\",\n",
    "    \"Quantum entanglement is a phenomenon where particles are interconnected and the state of one affects the state of another.\",\n",
    "    \"Quantum computers use qubits to perform complex calculations that classical computers struggle with.\"\n",
    "]\n",
    "references = [\n",
    "    \"A qubit is the fundamental unit of quantum computing.\",\n",
    "    \"Quantum entanglement is a physical phenomenon where particles become correlated in such a way that the state of one particle instantly influences the state of the other, no matter the distance between them.\",\n",
    "    \"A quantum computer uses qubits and quantum gates to perform computations that can solve certain problems more efficiently than classical computers by leveraging principles of quantum mechanics.\"\n",
    "]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = calculate_accuracy(reference_answers, model_responses)\n",
    "precision, recall, accuracy_token_based = calculate_precision_recall_accuracy(reference_answers, model_responses)\n",
    "metrics = compute_metrics(predictions, references)\n",
    "bleu_score = corpus_bleu(predictions, [references]).score\n",
    "context_precision, context_recall = compute_context_precision_recall(reference_answers, model_responses)\n",
    "hallucination_score = compute_hallucination(reference_answers, model_responses)\n",
    "answer_correctness = compute_answer_correctness(reference_answers, model_responses)\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "results = {\n",
    "    \"Metric\": [\n",
    "        \"Accuracy\", \"Precision\", \"Recall\", \n",
    "        \"Accuracy (Token-Based)\", \"F1 Score\", \n",
    "        \"Token-Based Accuracy\", \"BLEU Score\",\n",
    "        \"Context Precision\", \"Context Recall\",\n",
    "        \"Hallucination Score\", \"Answer Correctness\"\n",
    "    ],\n",
    "    \"Score (%)\": [\n",
    "        accuracy,\n",
    "        precision,\n",
    "        recall,\n",
    "        accuracy_token_based,\n",
    "        metrics['f1_score'],\n",
    "        metrics['token_based_accuracy'],\n",
    "        bleu_score,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        hallucination_score,\n",
    "        answer_correctness\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Display the results table using IPython.display with hidden index\n",
    "display(df_results.style.hide(axis='index')\n",
    "                      .set_table_attributes('style=\"font-size: 14px; text-align: center;\"')\n",
    "                      .set_caption(\"Evaluation Metrics\"))\n",
    "\n",
    "# Example error analysis\n",
    "def error_analysis(reference_answers, model_responses):\n",
    "    \"\"\"\n",
    "    Analyze and list the discrepancies between reference answers and model responses.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    for query, ref_answer in reference_answers.items():\n",
    "        model_answer = model_responses.get(query, \"\")\n",
    "        if model_answer.strip() != ref_answer.strip():\n",
    "            errors.append((query, ref_answer, model_answer))\n",
    "    return errors\n",
    "\n",
    "errors = error_analysis(reference_answers, model_responses)\n",
    "print(\"\\nError Analysis:\")\n",
    "for query, true_answer, pred_answer in errors:\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"True Answer: {true_answer}\")\n",
    "    print(f\"Model Answer: {pred_answer}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name='Knowledge Base',\n",
    "        func=qa.invoke,\n",
    "        description=(\n",
    "            'use this tool when answering general knowledge queries to get '\n",
    "            'more information about the topic'\n",
    "        )\n",
    "    )\n",
    "]\n",
    "from langchain.agents import create_react_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define the prompt template (if not already defined)\n",
    "template = '''Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}'''\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Create the React agent\n",
    "agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Optionally, you might use `AgentExecutor` if you need to manage interactions\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asking questions concerning the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'How does a quantum computer work?', 'output': 'A quantum computer works by utilizing the principles of quantum mechanics, which govern the behavior of particles at the atomic and subatomic levels. Unlike classical computers that use bits as the smallest unit of data (which can be either 0 or 1), quantum computers use quantum bits, or qubits. Qubits can exist in a state of 0, 1, or both simultaneously due to a property called superposition, allowing quantum computers to process vast amounts of information at once. Additionally, qubits can be entangled, meaning the state of one qubit can depend on the state of another, regardless of the distance between them. This entanglement enables quantum computers to perform complex calculations more efficiently than classical computers. Quantum computers manipulate the states of qubits through quantum gates, which are the quantum equivalent of classical logic gates. The combination of superposition, entanglement, and quantum gates allows quantum computers to solve certain problems much faster than classical computers, particularly those involving large datasets or complex simulations. However, quantum computing is still in its early stages, facing challenges such as maintaining coherence and reducing noise in quantum systems.'}\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Who is Michio Kaku?', 'output': 'Michio Kaku is a theoretical physicist, futurist, and popular science communicator. He is a professor of theoretical physics at the City University of New York and is known for his work in string theory and quantum physics. Kaku is also an author of several books, including \"Quantum Supremacy,\" which discusses the rise of quantum computers. He frequently appears in media to discuss scientific topics and the future of technology.'}\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"Who is Michio Kaku?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'How fast is a quantum computer?', 'output': 'Quantum computers can theoretically be infinitely faster than classical computers due to their use of qubits, which allow for simultaneous calculations. However, practical applications are still in development, and they do not consistently outperform classical computers across all tasks yet.'}\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"How fast is a quantum computer?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask a complete random question not related at all with the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'history of portugal in XV century?', 'output': \"The 15th century was a pivotal period in Portuguese history, marked by significant exploration and maritime expansion. Key events include:\\n\\n1. **Age of Discoveries**: Portugal became a leading maritime power, initiating the Age of Discoveries. This era saw explorers like Prince Henry the Navigator promoting voyages along the African coast.\\n\\n2. **Exploration of Africa**: Portuguese explorers reached the Azores and Madeira Islands, and they began to explore the West African coast, establishing trade routes and colonies.\\n\\n3. **Vasco da Gama**: In 1498, Vasco da Gama successfully sailed around the Cape of Good Hope to reach India, opening up a sea route that would enhance trade and establish Portuguese influence in Asia.\\n\\n4. **Colonization**: The 15th century also saw the beginning of Portuguese colonization in the Atlantic and the establishment of trading posts in Africa and Asia.\\n\\n5. **Treaty of Tordesillas**: In 1494, this treaty between Spain and Portugal divided newly discovered lands outside Europe, which significantly impacted the colonial ambitions of both nations.\\n\\nOverall, the 15th century laid the groundwork for Portugal's status as a global maritime empire in the following centuries.\"}\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"history of portugal in XV century?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import time\n",
    "\n",
    "# Initialize Pinecone client directly\n",
    "pinecone_client = pinecone.Pinecone(api_key=pinecone_api_key, environment='us-west1-gcp')\n",
    "\n",
    "index_name = 'langchain-multi-query'\n",
    "\n",
    "if index_name not in pinecone_client.list_indexes().names():  # Use .names() to get list of index names\n",
    "    # Define index configuration\n",
    "    index_spec = {\n",
    "        'dimension': 1536,\n",
    "        'metric': 'cosine'\n",
    "    }\n",
    "    pinecone_client.create_index(name=index_name, **index_spec)  # Pass index name as 'name'\n",
    "    while not pinecone_client.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pinecone_client.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3a2fba0abf4fe18ced6584b13f53cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "for i in tqdm(range(0, len(texts), batch_size)):\n",
    "    i_end = min(i+batch_size, len(texts))\n",
    "    ids = [str(uuid4()) for _ in range(i_end-i)]\n",
    "    embeds = embed.embed_documents(texts[i:i_end])\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas[i:i_end]))\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What is the speed of a quantum computer?', 'Can you tell me the velocity of a quantum computer?', 'How quickly can a quantum computer operate?']\n"
     ]
    }
   ],
   "source": [
    "text_field = \"text\"\n",
    "\n",
    "vectorstore = PineconeVectorStore(index, embed, text_field)\n",
    "llm = ChatOpenAI(temperature=0.0, openai_api_key=openai_api_key)\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=llm)\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "question = \"How fast is a quantum computer?\"\n",
    "\n",
    "texts = retriever.invoke(input=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In principle, a quantum computer is infinitely faster than a digital computer.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"contexts\"],\n",
    "    template=\"\"\"You are a helpful assistant who answers user queries using the contexts provided. If the question cannot be answered using the information provided say \"I don't know\".\n",
    "\n",
    "    Contexts:\n",
    "    {contexts}\n",
    "\n",
    "    Question: {query}\n",
    "    Answer:\"\"\"\n",
    ")\n",
    "\n",
    "qa_chain = LLMChain(llm=llm, prompt=QA_PROMPT, verbose=False)\n",
    "\n",
    "output = qa_chain(inputs={\"query\": question, \"contexts\": \"\\n---\\n\".join([d.page_content for d in texts])})\n",
    "print(output[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What is the speed of a quantum computer?', 'Can you tell me the velocity of a quantum computer?', 'What is the rate at which a quantum computer operates?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "In principle, a quantum computer is infinitely faster than a digital computer.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import TransformChain, SequentialChain\n",
    "\n",
    "def retrieval_transform(inputs: dict) -> dict:\n",
    "    texts = retriever.get_relevant_documents(query=inputs[\"question\"])\n",
    "    texts = [d.page_content for d in texts]\n",
    "    texts_dict = {\n",
    "        \"query\": inputs[\"question\"],\n",
    "        \"contexts\": \"\\n---\\n\".join(texts)\n",
    "    }\n",
    "    return texts_dict\n",
    "\n",
    "retrieval_chain = TransformChain(\n",
    "    input_variables=[\"question\"],\n",
    "    output_variables=[\"query\", \"contexts\"],\n",
    "    transform=retrieval_transform\n",
    ")\n",
    "\n",
    "rag_chain = SequentialChain(\n",
    "    chains=[retrieval_chain, qa_chain],\n",
    "    input_variables=[\"question\"],\n",
    "    output_variables=[\"query\", \"contexts\", \"text\"],\n",
    "    verbose=True\n",
    ")\n",
    "output = rag_chain({\"question\": question})\n",
    "print(output[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMChain output: {'question': 'What are the effects of climate change?', 'text': ['1. How does climate change impact global temperatures and weather patterns?', '2. What are the environmental consequences of climate change, such as rising sea levels and loss of biodiversity?', '3. In what ways does climate change affect human health and economies, both locally and globally?']}\n",
      "Type of question: <class 'str'>\n",
      "Content of question: How fast is a quantum computer?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What is the speed of a quantum computer compared to a traditional computer?', '2. Can a quantum computer solve complex problems faster than a classical computer?', '3. How does the processing speed of a quantum computer differ from a regular computer?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieved texts: 2\n"
     ]
    }
   ],
   "source": [
    "class SimpleListOutputParser(ListOutputParser):\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        # Split the text into lines and handle potential empty strings\n",
    "        return [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "output_parser = SimpleListOutputParser()\n",
    "\n",
    "template = \"\"\"\n",
    "Your task is to generate 3 different queries that aim to answer the user question from multiple perspectives.\n",
    "Every query MUST tackle the question from a different viewpoint, we want to get a variety of RELEVANT search results.\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0.3, openai_api_key=openai_api_key)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)\n",
    "\n",
    "# Debug: Test the LLMChain output\n",
    "test_question = \"What are the effects of climate change?\"\n",
    "result = llm_chain.invoke(test_question)\n",
    "print(\"LLMChain output:\", result)\n",
    "\n",
    "# Assuming vectorstore is defined elsewhere\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    llm_chain=llm_chain,\n",
    "    parser_key=\"lines\"  # This should match the attribute name in the output parser\n",
    ")\n",
    "\n",
    "# Debug: Print the type and content of 'question'\n",
    "print(\"Type of question:\", type(question))\n",
    "print(\"Content of question:\", question)\n",
    "\n",
    "texts = retriever.get_relevant_documents(query=question)\n",
    "print(\"Number of retrieved texts:\", len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMChain output: {'question': 'What are the effects of climate change?', 'text': ['1. How is quantum computing being used to study and address the effects of climate change?', '2. What role does AI play in predicting and mitigating the effects of climate change?', '3. What advancements in future technology are being developed to combat the effects of climate change?']}\n",
      "Type of question: <class 'str'>\n",
      "Content of question: How fast is a quantum computer?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the advantages of using a quantum computer over a traditional computer?', '2. How does the speed of a quantum computer compare to the speed of a supercomputer?', '3. Can quantum computing technology be used to solve complex problems faster than classical computing methods?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieved texts: 3\n"
     ]
    }
   ],
   "source": [
    "class SimpleListOutputParser(ListOutputParser):\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        # Split the text into lines and handle potential empty strings\n",
    "        return [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "output_parser = SimpleListOutputParser()\n",
    "\n",
    "template = \"\"\"\n",
    "Your task is to generate 3 different search queries that aim to answer the question from multiple perspectives. The user questions are focused on Quantum Computing, AI, future technology and related subjects.\n",
    "Every query MUST tackle the question from a different viewpoint, we want to get a variety of RELEVANT search results.\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0.3, openai_api_key=openai_api_key)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)\n",
    "\n",
    "# Debug: Test the LLMChain output\n",
    "test_question = \"What are the effects of climate change?\"\n",
    "result = llm_chain.invoke(test_question)\n",
    "print(\"LLMChain output:\", result)\n",
    "\n",
    "# Assuming vectorstore is defined elsewhere\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    llm_chain=llm_chain,\n",
    "    parser_key=\"lines\"  # This should match the attribute name in the output parser\n",
    ")\n",
    "\n",
    "# Debug: Print the type and content of 'question'\n",
    "print(\"Type of question:\", type(question))\n",
    "print(\"Content of question:\", question)\n",
    "\n",
    "texts = retriever.get_relevant_documents(query=question)\n",
    "print(\"Number of retrieved texts:\", len(texts))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0903c5ebc65c437c8e18c6676923c32a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1461f7546d3e49489d2c3488191fa0c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "202b7d48e97d407a8b2519f2644bc505": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5b08a1fbc75940448a0e4264353bdbd9",
       "IPY_MODEL_2614ff490ff4408f919a808b276148e9",
       "IPY_MODEL_6ae0cb2bebef45069e838c57bfeebb1f"
      ],
      "layout": "IPY_MODEL_8985e87dea3542a09736aed6d033eb70"
     }
    },
    "22b29699bfa544629f4cd136f6370fbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_869a2bd1ca924eb5bfa43ba403efb780",
       "IPY_MODEL_c675717db3744349b9fbef85bf398902",
       "IPY_MODEL_25cabc4efd04428e919cde59e3615ed3"
      ],
      "layout": "IPY_MODEL_9ca9b0300ead4a768340b5ca297e2374"
     }
    },
    "25cabc4efd04428e919cde59e3615ed3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aadc265d8186451faf96dc2dae45fcba",
      "placeholder": "​",
      "style": "IPY_MODEL_6bf3bd62bac2469f980a6181db55afe3",
      "value": " 10840000/? [00:54&lt;00:00, 247608.53it/s]"
     }
    },
    "2614ff490ff4408f919a808b276148e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65b0d5801c274e18943670076ab665e3",
      "max": 17,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5e3d765c4f7747fba62c02bd254fb6c8",
      "value": 17
     }
    },
    "2ff688dad44a4bcb91baf6e0b597a80d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3914abf7153a4d48b2f6a187e15012f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "582182f94da94f34b5a8470180d1f595": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9cc58f3194eb4d19a9cb13202361c518",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aa94e9f12b684778848d4735a38a5c54",
      "value": 1
     }
    },
    "5b08a1fbc75940448a0e4264353bdbd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d72b13aee7da4579bbeed8765650f5cb",
      "placeholder": "​",
      "style": "IPY_MODEL_c4f4ba0d51d44b9baa7ed6a413c1448d",
      "value": "100%"
     }
    },
    "5e3d765c4f7747fba62c02bd254fb6c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "60815b7b8320477e969cd67cb3aff844": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0903c5ebc65c437c8e18c6676923c32a",
      "placeholder": "​",
      "style": "IPY_MODEL_96681bb9a7c64238a7898c307b52744e",
      "value": " 1/1 [00:00&lt;00:00, 78.07it/s]"
     }
    },
    "65b0d5801c274e18943670076ab665e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ae0cb2bebef45069e838c57bfeebb1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91d4a8c0c8ff421885ab822efdafeba9",
      "placeholder": "​",
      "style": "IPY_MODEL_99b17916bbdd4680982b52501baface8",
      "value": " 17/17 [00:27&lt;00:00,  1.56s/it]"
     }
    },
    "6bf3bd62bac2469f980a6181db55afe3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "869a2bd1ca924eb5bfa43ba403efb780": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1461f7546d3e49489d2c3488191fa0c8",
      "placeholder": "​",
      "style": "IPY_MODEL_d58e937d585f43b9b680a2a86dd2eec3",
      "value": "Transcribing: "
     }
    },
    "8985e87dea3542a09736aed6d033eb70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91d4a8c0c8ff421885ab822efdafeba9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92cbdaf972004576951aa4a418fe4d83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce91347a876246fdbfb47461184423a3",
       "IPY_MODEL_582182f94da94f34b5a8470180d1f595",
       "IPY_MODEL_60815b7b8320477e969cd67cb3aff844"
      ],
      "layout": "IPY_MODEL_fec3f075ae2c4172a9199b80898e58f7"
     }
    },
    "96681bb9a7c64238a7898c307b52744e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98d50bbbafeb444295dae9fdaf9a3da7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "99b17916bbdd4680982b52501baface8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ca9b0300ead4a768340b5ca297e2374": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cc58f3194eb4d19a9cb13202361c518": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa94e9f12b684778848d4735a38a5c54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aadc265d8186451faf96dc2dae45fcba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4f4ba0d51d44b9baa7ed6a413c1448d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c675717db3744349b9fbef85bf398902": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ff688dad44a4bcb91baf6e0b597a80d",
      "max": 10836846,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_98d50bbbafeb444295dae9fdaf9a3da7",
      "value": 10836846
     }
    },
    "ce91347a876246fdbfb47461184423a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3914abf7153a4d48b2f6a187e15012f6",
      "placeholder": "​",
      "style": "IPY_MODEL_e08df7238a0344e5a0d535afe9ce97dc",
      "value": "100%"
     }
    },
    "d58e937d585f43b9b680a2a86dd2eec3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d72b13aee7da4579bbeed8765650f5cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e08df7238a0344e5a0d535afe9ce97dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fec3f075ae2c4172a9199b80898e58f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
